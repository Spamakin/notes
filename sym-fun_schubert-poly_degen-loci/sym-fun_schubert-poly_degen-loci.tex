\documentclass[letterpaper, 11pt, oneside]{book}

\usepackage{style}  % If you feel like procrastinating, mess with this file
\usepackage{algo}   % Thank you Jeff, very cool!

\addbibresource{refs.bib}

% Required reading
% https://jmlr.csail.mit.edu/reviewing-papers/knuth_mathematical_writing.pdf
%   Along with required viewing:
%   https://www.youtube.com/watch?v=N6QEgbPWUrg&list=PLOdeqCXq1tXihn5KmyB2YTOqgxaUkcNYG
% https://faculty.math.illinois.edu/~west/grammar.html

% % % % % % % % % %
%     Cursor      %
%     Parking     %
%     Lot         %
% % % % % % % % % %

% Disable check for mismatched parens/brackets/braces
%   chktex-file 9
% Disable check for different counts of parents/brackets/braces
%   chktex-file 17
% Exclude these environments from syntax checking
%   VerbEnvir { tikzcd }

\regtotcounter{figure}

\title{\vspace{-100pt} {\Huge Young Tableau, Symmetric Functions, \protect\\ Schubert Polynomials, and Degeneracy Loci} \\ {\small With 0 Figures}}
\author{\Large Anakin Dey}
\DTMsavenow{now}
\date{\small Last Edited on \today\ at \DTMfetchhour{now}:\DTMfetchminute{now}}

% Cover page number chicanery
\newcommand{\CoverName}{Cover}

\begin{document}
\frontmatter
\renewcommand{\thepage}{\CoverName}
\maketitle

\pagenumbering{roman}

\tableofcontents
\clearpage


% \listoftheorems[ignoreall, show={defn}, title={List of Definitions}]
%
% \listoftheorems[ignoreall, show={ex}, title={List of Examples and Counterexamples}]

\chapter*{Preface}

These are notes for a reading course under Professor \href{https://people.math.osu.edu/anderson.2804/index.html}{Dave Anderson}.
The primary focus is Manivel's \emph{Symmetric Functions, Schubert Polynomials, and Degeneracy Loci}~\cite{book:Manivel} which one could see as a quasi-sequel to Fulton's \emph{Young Tableaux}\footnote{which throughout these notes will be spelled as ``tableaux'' or ``tableau'' with no real consistency.}~\cite{book:YT}.
Primarily, the solutions will be to exercises from~\cite{book:Manivel}.
However, as needed there will be solutions to material from~\cite{book:YT}, or perhaps even other texts such as~\cite{book:MacdonaldSymmetricHall} or~\cite{book:ECII}.

\mainmatter

\chapter{\cite{book:YT} Geometry}

\begin{sol}[\cite{book:YT} \S 9.1 Ex. 1]\label{ex:YT_9.1.1}
  Choose a basis $\set{e_{1}, \ldots, e_{m}}$ so that $E$ can be identified with $\C^{m}$.
  Let $i_{1} < \cdots < i_{d - 1}$ and $j_{1} < \cdots j_{d + 1}$ be sequences in $[m]$.
  Apply \S 9.1 Equation (1) with $k = 1$ to the sequences $j_{2} < \cdots < j_{d + 1}$ and $i_{1} < \cdots < i_{d - 1}, j_{1}$ by fixing $j_{1}$ to be the vector swapped successively with the $j_{2} < \cdots < j_{d + 1}$.
  Reordering the indices and applying the appropriate sign change yields the desired alternating summation.
\end{sol}

\begin{sol}[\cite{book:YT} \S 9.1 Ex. 2]\label{ex:YT_9.1.2}
  We have that $V \subseteq E = \C^{4}$ is given as the kernel of multiplication of a matrix $A = (a_{i, j})_{\substack{1 \leq i \leq 4 \\ 1 \leq j \leq 2}}$.
  To find this matrix, the given conditions of the $x_{i, j}$ describe the following determinantal conditions on the entries of $A$:
  \begin{align*}
    x_{1, 2} = 1 &\iff \Delta_{1, 2}(A) = 1, \\
    x_{1, 3} = 2 &\iff \Delta_{1, 3}(A) = 2, \\
    x_{1, 4} = 1 &\iff \Delta_{1, 4}(A) = 1, \\
    x_{2, 3} = 1 &\iff \Delta_{2, 3}(A) = 1, \\
    x_{2, 4} = 2 &\iff \Delta_{2, 4}(A) = 2, \\
    x_{3, 4} = 3 &\iff \Delta_{3, 4}(A) = 3.
  \end{align*}
  From here, we must make an assumption based on which affine portion of $\mathbb{P}^{5}$ our matrix lives in.
  This amounts to picking some $i_{1}, i_{2}$ so that the minor given by those columns is the identity matrix.
  For the given conditions, we could pick $(i_{1}, i_{2}) = (1, 2)$, $(1, 4)$, or $(2, 3)$.
  We give $A$ for each of these choices respectively:
  \begin{align*}
    A =
    \begin{pmatrix}
      1 & 0 & -1 & -2 \\
      0 & 1 & 2 & 1
    \end{pmatrix},
    &&
    A =
    \begin{pmatrix}
      1 & 2 & 3 & 0 \\
      0 & 1 & 2 & 1
    \end{pmatrix},
    &&
    A =
    \begin{pmatrix}
      2 & 1 & 0 & -3 \\
      -1 & 0 & 1 & 2
    \end{pmatrix}.
  \end{align*}
  It is clear that these are the same matrix up to row operations.
\end{sol}

\begin{sol}[\cite{book:YT} \S 9.1 Ex. 4]\label{ex:YT_9.1.4}
  Following~\cite[Corollary of Theorem 1, \S 8.1]{book:YT}, we have that $S^{\bullet}(m; d_{1}, \ldots, d_{s})$ is canonically isomorphic to the subalgebra of $\C[Z]$ generated by all $D_{T}$, where $T$ varies over all tableaux on Young diagrams whose columns have lengths in $\set{d_{1}, \ldots, d_{s}}$ and entries in $[m]$ where
  \[
    D_{T} = \prod_{j = 1}^{\ell} D_{T(1, j), T(2, j), \ldots, T_{\mu_{j}, j}}
  \]
  where $\mu_{j}$ is the length of the $j\textsuperscript{th}$ column of $\lambda$ the shape of $T$ and $\ell = \lambda_{1}$.
  \begin{enumerate}[label=(\alph*)]
    \item We mimic the proof of~\cite[Proposition 2, \S 9.1]{book:YT}.
          \quest{I think this proof needs to be rewritten, perhaps with a highest weight argument?}
          Let $G = G(d_{1}, \ldots, d_{s}) \leq \GL(V)$.
          The dimension of the vector space of polynomials of homogeneous polynomials of degree $a$ in the span of all the $D_{i_{1}, \ldots, i_{p}}$ for $p \in \set{d_{1}, \ldots, d_{s}}$ is $\sum d_{\lambda}(m)$ where the sum ranges over all partitions of $a$ of shape $\lambda$ with columns whose lengths lie in $\set{d_{1}, \ldots, d_{s}}$.
          Viewing $V^{\oplus m}$ by identifying $Z_{i, j}$ with the $i\textsuperscript{th}$ basis vector of the $j\textsuperscript{th}$ copy of $V$, we have by~\cite[Corollary 3(a), \S 8.3]{book:YT} that $\C[Z]_{a} = \Sym^{a}(V^{\oplus m}) \simeq \bigoplus_{\lambda \vdash a} (V^{\lambda})^{d_{\lambda}(m)}$ where $\lambda \vdash a$ has at most $n$ rows.
          Thus, we would like to show that $(V^{\lambda})^{G}$ has dimension $1$ when the lengths of the columns of $\lambda$ lie in $\set{d_{1}, \ldots, d_{s}}$ and $0$ otherwise.

          We recall the construction of $V^{\lambda}$ in \S 8.1 of~\cite{book:YT}.
          Elements of $V^{\times \lambda}$ are specified by specifying an element of $V$ for each box in $\lambda$.
          Fillings by basis vectors $\set{e_{1}, \ldots, e_{n}}$ corresponding to semistandard Young Tableaux $T$ of shape $\lambda$ with entries in $[n]$.
          The images of such elements in $V^{\times \lambda}$ in $V^{\lambda}$ form a basis $\set{e_{T}}$ of $V^{\lambda}$.
          Consider the basis element corresponding to the tableaux $U(\lambda)$ given by filling every box on row $i$ with the number $i$.
          For maps in $G$, the first $d_{i}$ basis vectors must map to linear combinations of the first $i$ basis vectors and the restrictions of such maps to the $V_{i}$ have determinant $1$.
          As such, we can only consider $\lambda$ whose columns have lengths lying in $\set{d_{1}, \ldots, d_{s}}$.
          To see that $e_{U(\lambda)}$ is the only such fixed basis vector,
    \item
  \end{enumerate}
\end{sol}


\clearpage

\chapter{\cite{book:Manivel} The Ring of Symmetric Functions}

\section{Ordinary Functions}

\begin{sol}[\cite{book:Manivel} Ex. 1.1.2]\label{ex:Manivel_1.1.2}
  We will denote the dominance ordering by $\lambda \leq \mu$ and the ordering given by inclusion of Ferrers diagrams by $\lambda \subseteq \mu$.
  Let $\lambda = (\lambda_{1} \geq \cdots \geq \lambda_{k} \geq 0)$ and $\lambda' = (\lambda'_{1} \geq \cdots \geq \lambda'_{l} \geq 0)$ be two partitions.

  We first consider the ordering $\subseteq$.
  Note that $\lambda \subseteq \lambda'$ if and only if $k \leq l$ and for all $1 \leq i \leq k$ we have that $\lambda_{i} \leq \lambda'_{i}$.
  Let $m = \min\set{k, l}$.
  Then define a partition $\mu = (\min\set{\lambda_{1}, \lambda'_{1}} \geq \cdots \min\set{\lambda_{m}, \lambda'_{m}} \geq 0)$.
  Then we have that $\mu \subseteq \lambda$ and $\mu \subseteq \lambda'$.
  Now suppose that $\nu \subseteq \lambda$ and $\nu \subseteq \lambda'$ where $\nu = (\nu_{1} \geq \cdots \geq \nu_{n} \geq 0)$.
  Then we must have that $n \leq \min\set{k, l} = m$ and that for all $1 \leq i \leq n$ that $\nu_{i} \leq \min\set{\lambda_{i}, \lambda'_{i}} = \mu_{i}$.
  Thus, $\nu \subseteq \mu$ and so $\mu = \lambda \land \lambda'$ with respect to $\subseteq$.
  The existence and uniqueness of $\lambda \lor \lambda'$ is similar.

  We now consider the ordering $\leq$, now assuming that $\abs{\lambda} = \abs{\lambda'}$.
  Before we define $\lambda \lor \lambda'$ for $\leq$, we prove that $\lambda \leq \lambda'$ if and only if ${\lambda'}^{*} \leq \lambda^{*}$.
  This follows a proof given by~\cite{SE:dominance_conjugation}.
  Note that $\lambda \leq \lambda'$ if and only if $\lambda$ can be obtained from $\lambda'$ by moving boxes successively down from higher rows to lower rows such that every intermediate diagram is still a Ferrers diagram.
  This immediately implies the duality.

  First, for a partition $\lambda$ let $\hat{\lambda} \defeq (0, \lambda_{1}, \lambda_{1} + \lambda_{2}, \ldots, \lambda_{1} + \cdots + \lambda_{k})$.
  We remark that $\lambda \leq \lambda'$ if and only $\hat{\lambda} \leq_{\ell} \hat{\lambda'}$ where $\leq_{\ell}$ is \emph{lexicographic ordering}.
  One can easily recover $\lambda$ from $\hat{\lambda}$.
  By taking componentwise minimums as above for $\hat{\lambda}$ and $\hat{\lambda'}$, one recovers a tuple $\hat{\mu}$ which yields a partition $\mu$.
  By the remark, we have that $\mu = \lambda \land \lambda'$ with respect to $\leq$.
  Then to define $\lambda \lor \lambda'$, we have that $\lambda \lor \lambda' \defeq (\lambda^{*} \land {\lambda'}^{*})^{*}$.
  That this is our desired maximum follows from the above paragraph on conjugation being a lattice antiautomorphism on partitions of the same weight.
  Uniqueness is also immediate.
\end{sol}

\clearpage

\begin{sol}[\cite{book:Manivel} Ex. 1.1.7]\label{ex:Manivel_1.1.7}
  These ideas come from~\cite[Proposition 7.4.1]{book:ECII}.
  Let $X = (x_{ij})$ be the matrix of variables where $x_{ij} = x_{j}$, so the first column of $X$ is all $x_{1}$, the second column is all $x_{2}$, etc.
  We can obtain a term from of $e_{\lambda}$ from $X$ by choosing $\lambda_{1}$ elements from the first row, $\lambda_{2}$ elements from the second row, corresponding to picking a term from $e_{\lambda_{1}}$, then a term from $e_{\lambda_{2}}$, etc.
  After choosing all elements, let the result be $\overline{x}^{\alpha}$.
  Replace all the chosen elements with $1$'s and all the other elements with $0$'s.
  This gives a matrix with row sums given by $\lambda$ and all column sums given by $\alpha$.
  Note that $\alpha$ is not necessarily a partition, but rather just a tuple (also called a \emph{weak composition}), which is in line with the fact that monomial symmetric functions are sums over arbitrary orderings of tuples with a fixed set of entries.
  Conversely, any such $0$-$1$1matrix with the prescribed row and column sums describes a term of $e_{\lambda}$.
  Thus, we have that $e_{\lambda} = \sum_{\mu} a_{\lambda \mu} m_{\mu}$.

  Similarly, with $X$ as before, we can obtain a term of $h_{\lambda}$ as follows.
  Choose $\lambda_{1}$ elements from the first row, but we allow each term to be chosen more than once.
  Next, choose $\lambda_{2}$ elements from the second row, again allowing for each term to be chosen more than once.
  Continuing on in this manner yields a term $\overline{x}^{\alpha}$.
  This again give a matrix, however this time with entries in $\N$ given by numbering the elements with however many times they were chosen.
  This matrix has the prescribed row and column sums.
  Conversely, any such matrix with entries in $\N$ with the given row and column sums gives a term of $h_{\lambda}$ and so $h_{\lambda} = \sum_{\mu} b_{\lambda \mu} m_{\mu}$.

  Now suppose that $a_{\lambda \mu} > 0$.
  Then we want to show that $\mu \leq \lambda^{*}$,  \ie\ that $\abs{\lambda} = \abs{\mu}$ and that for all $i$ we have that $\mu_{1} + \cdots + \mu_{i} \leq \lambda^{*}_{1} + \cdots + \lambda^{*}_{i}$.
  If $\abs{\lambda} \neq \abs{\mu}$, then we must have that $a_{\lambda \mu} = 0$ as both $\abs{\lambda}$ and $\abs{\mu}$ are equal to the total number of ones and so we must have that $\abs{\lambda} = \abs{\mu}$.
  So by the above argument, there exist a $0$-$1$-matrix $M$ with row sums given by $\lambda$ and column sums given by $\mu$.
  Suppose there exists $i$ such that $\mu_{1} + \cdots + \mu_{i} > \lambda_{1}^{*} + \cdots + \lambda_{i}^{*}$.

  \quest{Morally} I would like to say the $\lambda_{i}^{*}$ correspond to column sums as well in some manner but I am not sure how to phrase that.
\end{sol}

\clearpage

\section{Schur Functions}

\begin{sol}[\cite{book:Manivel} Ex. 1.2.4]\label{ex:Manivel_1.2.4}
  We have that $a_{\delta + \delta} = \det(x_{i}^{\delta_{j} + n - j}) = \det(x_{i}^{2n - 2j})$.
  This is the Vandermonde determinant again, but now every term is squared.
  Thus, $a_{\delta + \delta} = \prod_{1 \leq i < j \leq n} (x_{i}^{2} - x_{j}^{2})$.
  Thus, we have that
  \[
    s_{\delta} = \frac{a_{\delta + \delta}}{a_{\delta}} = \frac{\prod_{1 \leq i < j \leq n} (x_{i}^{2} - x_{j}^{2})}{\prod_{1 \leq i < j \leq n} (x_{i} - x_{j})} = \prod_{1 \leq i < j \leq n} (x_{i} + x_{j}).
  \]
\end{sol}

\begin{sol}[\cite{book:Manivel} Ex. 1.2.7]\label{ex:Manivel_1.2.7}
  By Pieri's formulas, we have that
  \begin{equation}\label{eq:Pieri_even_expansion}
    \pqty{\sum_{\mu \text{ even}} s_{\mu}} \cdot \pqty{\sum_{n = 0}^{k} e_{k}} = \sum_{\mu \text{ even}} \sum_{k = 0}^{n} s_{\mu} e_{k} = \sum_{\mu \text{ even}} \sum_{k = 0}^{n} \sum_{\lambda \in \mu \otimes 1^{k}} s_{\lambda}.
  \end{equation}
  Clearly, every $s_{\lambda}$ term, \emph{not monomial terms}, in the last summation of \Cref{eq:Pieri_even_expansion} is a term in $\sum_{\lambda} s_{\lambda}$, except possibly with a coefficient $> 1$.
  We claim that all the coefficients are indeed $1$ and that every term in $\sum_{\lambda} s_{\lambda}$ appears in the in the last summation of \Cref{eq:Pieri_even_expansion}.
  This follows from the fact that for any $\lambda$, we can decompose $\lambda$ into an even $\mu$ by removing at most one box from each row of $\lambda$ in each row which is odd and that this removal is unique.
\end{sol}

\begin{sol}[\cite{book:Manivel} Ex. 1.2.12]\label{ex:Manivel_1.2.12}
  The first identity comes from noticing that if you take any standard Young tableaux with $n$ boxes and remove the box labelled $n$, then you obtain a standard Young tableaux with $n - 1$ boxes.
  Furthermore, if you add a box labelled $n$ to any valid position of a Young tableaux with $n - 1$ boxes, valid meaning the resulting shape is still a partition, then you obtain a standard Young tableaux with $n$ boxes.
  This gives a combinatorial bijection between the two sets described by each side of first identity.

  For the second identity, suppose that $\abs{\lambda} = (1)$.
  Then $\lambda$ is just a single box and thus we must have that $K_{\lambda} = K_{(1)} = 1$ and so $(1 + \abs{(1)})K_{(1)} = 2$.
  Then $(1) \otimes 1 = \set{(1, 1), (2)}$ which each have exactly one standard filling and so we have that $K_{(1, 1)} = K_{(2)} = 1$ and thus $\sum_{\mu \in (1) \otimes 1} K_{\mu} = 2$.
  Now suppose that $\abs{\lambda} = n > 1$.
  We have that
  \begin{align*}
    (1 + \abs{\lambda})K_{\lambda} &= (1 + \abs{\lambda}) \sum_{\lambda \in \mu \otimes 1} K_{\mu} \\
                                   &= \sum_{\lambda \in \mu \otimes 1} K_{\mu} + \sum_{\lambda \in \mu \otimes 1} (1 + \abs{\mu})K_{\mu}
                                   &= \sum_{\lambda \in \mu \otimes 1} K_{\mu} + \sum_{\lambda \in \mu \otimes 1} \sum_{\nu \in \mu \otimes 1} K_{\nu}
  \end{align*}
  \quest{Not sure} how to work with this double summation.

  \clearpage

  For the third identity, as $K_{(1)} = 1$ we immediately have that $\sum_{\lambda = 1} K_{\lambda}^{2} = K_{(1)}^{2} = 1 = 1!$.
  Now suppose that $\abs{\lambda} = \ell > 0$.
  Then we have that
  \begin{align*}
    \ell! &= \ell \cdot (\ell - 1)! \\
          &= \ell \sum_{\abs{\lambda} = \ell - 1} K_{\lambda}^{2} \\
          &= \sum_{\abs{\lambda} = \ell - 1} K_{\lambda} \cdot (\ell K_{\lambda}) \\
          &= \sum_{\abs{\lambda} = \ell - 1} K_{\lambda} \cdot \sum_{\mu \in \lambda \otimes 1} K_{\mu}\\
  \end{align*}
  \quest{Not sure} how to work with this double summation.
\end{sol}

\begin{sol}[\cite{book:Manivel} Ex. 1.2.15]\label{ex:Manivel_1.2.15}
  Recall that $h_{j} = s_{(j)}$ and $e_{k} = s_{1^{k}}$.
  Using the Pieri formulas, we can express $h_{j}e_{k}$ as
  \[
       \sum_{\mu \in 1^{k} \otimes j} s_{\mu} = s_{1^{k}}h_{j} = h_{j}e_{k} = s_{(j)}e_{k} = \sum_{\mu \in (j) \otimes 1^{k}} s_{\mu}.
  \]
  \quest{Expanding either side} gives $h_{j}s_{k} = s_{(j - 1 \mid k)} + s_{(j \mid k - 1)}$ which is already stated.
  Not sure what a second way would be, nor how to introduce the variable $q$ in a generating-function sort of way.
\end{sol}

\clearpage

\section{The Knuth Correspondence}

\begin{sol}[\cite{book:Manivel} Ex. 1.3.1]\label{ex:Manivel_1.3.1}
  Already saw this as the \emph{Row Bumping Lemma} in~\cite{book:YT} which gives a slightly stronger characterization.
\end{sol}

\section{Some Applications to Symmetric Functions}

\begin{sol}[\cite{book:Manivel} Ex. 1.4.4]\label{ex:Manivel_1.4.4}
  \quest{Why} are these bases?

  Let $M_{\lambda \nu}$ and $N_{\lambda \nu}$ be such that $s_{\lambda} = \sum_{\mu} M_{\lambda \mu} = \sum_{\nu} N_{\lambda \nu} b_{\nu}$.
  Then we have that
  \begin{align*}
    \sum_{\lambda} a_{\lambda}(\overline{x}) b_{\lambda}(\overline{y}) &= \prod_{i, j} (1 - x_{i}y_{j})^{-1} \\
                                                                       &= \sum_{\lambda} s_{\lambda}(\overline{x})s_{\lambda}(\overline{y}) \\
                                                                       &= \sum_{\lambda} \pqty{\sum_{\rho} M_{\lambda \rho} a_{\rho}(\overline{x})} \pqty{\sum_{\nu} N_{\lambda \nu} b_{\nu}(\overline{y})} = \sum_{\rho, \nu} \pqty{\sum_{\lambda} M_{\lambda \rho} N_{\lambda \nu}} a_{\rho}(\overline{x}) b_{\nu}(\overline{y}).
  \end{align*}
  Thus by the fact that the $a_{\rho}$ and $b_{\nu}$ form bases in their respective variables, we have that $\sum_{\lambda} M_{\lambda \rho} N_{\lambda \nu} = \inner{a_{\rho}, b_{\nu}}$.
  We want to show that $\inner{a_{\rho}, b_{\nu}} = \delta_{\rho \nu}$.
  Indeed, this follows from that
  \[
    \sum_{\lambda} s_{\lambda}(\overline{x}) s_{\lambda}(\overline{y}) = \sum_{\lambda} a_{\lambda}(\overline{x}) b_{\lambda}(\overline{y}) \implies \sum_{\lambda} M_{\lambda \rho} N_{\lambda \nu} = \delta_{\rho \nu}.
  \]
\end{sol}

\clearpage

\section{The Littlewood-Richardson Rule}

\begin{sol}[\cite{book:Manivel} Ex. 1.5.4]\label{ex:Manivel_1.5.4}
  We consider the coefficient of $\overline{x}^{\alpha}$ on both sides.
  We have that
  \[
    \prod_{i} (1 - x_{i})^{-1} \cdot \prod_{i < j} (1 - x_{i}x_{j})^{-1} = \pqty{\prod_{i} \sum_{n \geq 0} x_{i}^{n}} \cdot \pqty{\prod_{i < j} \sum_{n \geq 0} x_{i}^{n}x_{j}^{n}}.
  \]
  Notices that the coefficient of $\overline{x}^{\alpha}$ is equal to the number of symmetric matrices $A$ such that the vector of row-sums of $A$ is equal to $\alpha$.
  Then, by the combinatorial definition of Schur polynomials, the coefficient of $\overline{x}^{\alpha}$ in $\sum_{\lambda} s_{\lambda}(\overline{x})$ is equal to the number of semistandard Young tableaux with weight vector $\alpha$.
  Then by~\cite[Knuth Correspondence 1.3.4]{book:Manivel} and in particular~\cite[Corollary 1.5.3]{book:Manivel}, we know these two quantities must be equivalent, and thus the identity holds.
\end{sol}

\printbibliography
\end{document}
